---
title: "Random Forest (Decision Trees)"
#author ref https://quarto.org/docs/journals/authors.html
author: 
- name: "Danielle Novinski"
- name: "Victor Richerson"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

This is an introduction to random forests, a machine learning method widely used in a variety of applications such as ecology, medicine, astronomy, agriculture, traffic, and bioinformatics [@Fawagreh2014]. Random forest is an ensemble learning method composed of multiple decision trees. Breiman [@Breiman2001] proposed random forests to improve on decision trees for regression and classification tasks. Since this initial paper, many studies have been proposed to improve and extend the random forest method as well as use random forest for classification and regression techniques in a variety of applications. Our paper will provide an overview of the existing research and methods used.

<!--
This is an introduction to Random Forests.  Random forests are ensembles of decision trees that incorporate a randomization feature to increase predictive accuracy.  The method is attributed to Dr. Leo Breiman [@Breiman2001] of the University of Berkeley in California.  Dr. Breiman is well known and has prior work with Classification and Regression Trees, a textbook on decision tree methodology.  Dr. Breiman previously worked on expanding decision trees with the Bagging Predictors methodology [@Breiman1996] and was exposed to the work of other researchers with the AdaBoost (adaptive boosting) methodology.  The random forest methodology has a basis in randomization ideas presented by Thomas G. Dietterich of Oregon State University [@Dietterich2000].  The random forest method of using ensembles of randomized decision trees has been a popular research topic with many other researchers looking into ways to expand on and improve the method.  "Rotation Forest" was a method to incorporate random forest with PCA to create new features [@Rodriguez2006].  This method was just one of many to follow on and improve the original random forest methodology.
-->

## Literature Review

### Background

Decision trees are used for predictions and classification in a variety of artificial intelligence and machine learning tasks. Decision trees divide data by fields, creating subsets called nodes. Cut offs are applied based on statistics. [@deVille2013] Researchers sought to make improvements to decision tree classifiers, such as reducing bias and variance. Resampling training data and using multiple trees reduces bias and variance [@deVille2013]. Bootstrap aggregating (bagging) and AdaBoost were improvements over the standard decision tree. In addition, other multi-tree solutions came before the random forest method, including voting over multiple trees and using split methods of alternative splits and oblique splits. [@Breiman1996].

Dietterich [@Dietterich2000] compared a method of randomizing split of C4.5 decision tree method with AdaBoost and Bagging methods on 33 datasets. This study discussed the effect of randomization leading to a diverse set of decision trees. The performance with “noise” or changed class labels for percentages of the data was also tested since it was noted that AdaBoost did not do well with such noise, due to AdaBoost amplifying the effect of the noise. Rodriguez, et. al. [@Rodriguez2006] discuss bagging, boosting, and random forest methods.  Mentions that adaboost is “equivalent to fitting an additive logistic regression model by a stage-wise estimation procedure,” and mentions that some of the math behind bounds and generalization error have been proven, providing references. Mentions that AdaBoost was successful because of imparting diversity to the ensemble, but there was a tradeoff in accuracy.  

Breiman's paper, “Random Forests”, [@Breiman2001] is attributed to be the first mention and description of the random forest technique.  Random forest classifiers are based on decision trees which have been generated from random subsets of the features (columns of the data, considering that rows are observations of the data).  The classification is determined by majority vote of the decision trees by row.  Random Forest can be used for classification (predicting class label) or regression (predicting continuous value).  An advantage of random forest is its performance with noisy data, since it is not weighting the data and hence potentially weighting noise.  Random forests do not overfit data due to the randomness of the selecting variables to use for the prediction.  The “out of bag” estimate for a class label is the estimate using trees that do not contain the class label, and it is a good estimate of overall error rate.  Random forests can have similar error rates as another machine learning algorithm, AdaBoost. 

Random forest is an ensemble method. Ensemble learning methods use several methods to solve a problem. Diversity among methods is indicated by methods making different errors. This leads to improvements in classifications [@Fawagreh2014]. 

### Improvements and Expansions to Random Forest

Random forest extensions include non-parametric test of significance, weighted voting, dynamic integration, weighted random sampling, online random forest, and genetic algorithms [@Fawagreh2014]. Verikas et. al [@Verikas2011] found that random forest is sensitive to tuning parameters such as number of variables selected and number of trees. Boulesteix et. al [@Boulesteix2012] suggested modifications to the random forest approach regarding how individual trees are built, the construction of datasets, and how individual predictions are coalesced into one prediction. For example, conditional inference forests are random forests where the predictors of each split are tested for response and allow adjustment for different predictors. 

For parameter tuning, if the number of trees increases with the number of predictors, the randomization process will lead to predictors having a good chance to be selected. Predictor selection is important. Some predictors are not informative. The number of predictors is important in order to moderate the effects of strong and weak predictors. The size of trees is determined by the splitting criteria. The size of the leaves is another parameter wherein good predictors could be overlooked if the leaf size threshold is too small. The authors suggest sampling without replacement to avoid bias, and sampling a certain number of observations from multiple classes [@Boulesteix2012]. 

A study by Gardener et. al [@Gardner2021] combined Random Forest methodology with PCA to improve precision.  PCA is used to create new combination vectors for dependent features in this case.  It is ideal to create a new feature when a dependency is found, but this methodology would take the guesswork out of it and automate the creation of the new features.  The random forest technique would then incorporate the new features.  This allows the single split of a random forest / decision tree to split on a variable that combines multiple features.  Another gain is the fact that each tree has unique features due to the PCA being applied on each tree, so the trees are less correlated, which has been shown to increase accuracy.

Rodriguez et. al [@Rodriguez2006] also attempted to improve on random forest methods by using PCA to create new features in a “rotation” method to classify the rotated data. The authors mention that rotation may not be optimal for feature extraction and there are other linear transformations/projections that could be used instead of PCA. This study used disjoint sets of data to create different classifiers and that it could use intersecting sets. PCA yields some zero eigenvectors and therefore the number of components varies.  This study kept all the components rather than discard low ranking ones. The authors found that the rotation forest method has more accurate but less diverse (but still reasonably diverse) individual classifiers and that contrasts with random forests having more diverse but less accurate classifiers, concluding that the rotation method tested provided superior results.

Breiman in 2004 [@Breiman2004] attempted to create some simple random forest models where mathematical consistency could be proven, referring to a paper by Yi Lin & Yongho Jeon in 2002 (published 2006) [@Lin2006], that showed that the random forest method was a form of adaptive nearest neighbors, as the tree or split is a form of distance measure and therefore it is a nearest neighbor approach. The concept of strong and weak variables was introduced, as well as attempts to create equations to bound variance and bias for this simple model to prove consistency.  This introduces the question: problems themselves aren’t always consistent so can the solutions be consistent?

### Applications

As the size of datasets increases (defined by the number of variables exceeding the number of observations), the performance of statistical methods declines; machine learning methods are preferred [@Verikas2011]. Random forests have been used in several studies related to biology and medicine. In such applications, there is usually a relationship between response and predictor variables and sometimes strong correlations between predictors. Random forests have helped with prediction, ranking of influential variables, and identifying variables with interactions. Other studies have found good performance in areas such as predicting customer churn, fraud detection, identifying bacteria and fish, and identifying eye disease [@Verikas2011]. Lin et al [@Lin2019] analyzed a very large dataset in order to determine risk factors for diabetes, a disease affecting 425 million adults where chronic high blood sugar has effects such as damaging organs. A study by Beaulac and Rosenthal [@Beaulac2019] focused on prediction of students' major and completion status, finding they were able to predict program completion with 78.84% accuracy and predict choice of major with 47.41% accuracy. [@Moore2019] used random forest to predict conversion from MCI (mild cognitive impairment) to AD (Alzheimer's disease) through pairwise comparison of time series data. The authors used a random forest on time series with four data points and 60 trees, yielding 90% accuracy for the random forest method classification of neuroimage data of AD vs HC (healthy control). 



## Methods

<!--
The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.
-->

## Analysis and Results

### Data and Vizualisation

<!--
A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```
-->

### Statistical Modeling

### Conlusion

## References

<!--
@article{wang2014,
  title={Generalized estimating equations in longitudinal data analysis: a review and recent developments},
  author={Wang, Ming},
  journal={Advances in Statistics},
  volume={2014},
  year={2014},
  publisher={Hindawi}
}

@Manual{R-base,
  title = {R: A Language and Environment for Statistical
           Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2019},
  url = {https://www.R-project.org},
}

@book{efr2008,
  title={Nonparametric Curve Estimation: Methods, Theory, and Applications},
  author={Efromovich, S.},
  isbn={9780387226385},
  lccn={99013253},
  series={Springer Series in Statistics},
  url={https://books.google.com/books?id=mdoLBwAAQBAJ},
  year={2008},
  publisher={Springer New York}
}
@article{bro2014principal,
  title={Principal component analysis},
  author={Bro, Rasmus and Smilde, Age K},
  journal={Analytical methods},
  volume={6},
  number={9},
  pages={2812--2831},
  year={2014},
  publisher={Royal Society of Chemistry}
}
-->