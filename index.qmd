---
title: "Random Forest (Decision Trees)"
#author ref https://quarto.org/docs/journals/authors.html
author: 
- name: "Danielle Novinski"
- name: "Victor Richerson"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

This is an introduction to random forests, a machine learning method
widely used in a variety of applications such as ecology, medicine,
astronomy, agriculture, traffic, and bioinformatics [@Fawagreh2014].
Random forest is an ensemble learning method composed of multiple
decision trees. Breiman [@Breiman2001] proposed random forests to
improve on decision trees for regression and classification tasks. Since
this initial paper, many studies have been proposed to improve and
extend the random forest method as well as use random forest for
classification and regression techniques in a variety of applications.
Our paper will provide an overview of the existing research and methods
used.

## Literature Review

### Background

Decision trees are used for prediction and classification in a variety
of artificial intelligence and machine learning tasks. Decision trees
divide data by fields, creating subsets called nodes. Cutoffs are
applied based on statistics [@deVille2013]. Researchers have sought to
make improvements to decision tree classifiers, such as reducing bias
and variance. Resampling training data and using multiple trees reduces
bias and variance [@deVille2013]. Bootstrap aggregating (bagging) and
AdaBoost were improvements over the standard decision tree. In addition,
other multi-tree solutions came before the random forest method,
including voting over multiple trees and using split methods of
alternative splits and oblique splits [@Breiman1996]. The Random Forest
method is an ensemble method. Ensemble methods use several methods to
solve a problem. Diversity among methods is indicated by methods making
different errors. This leads to improvements in classifications
[@Fawagreh2014].

Dietterich [@Dietterich2000] compared a method of randomizing the split
of C4.5 decision trees with AdaBoost and Bagging methods on 33 datasets.
This study discussed the effect of randomization leading to a diverse
set of decision trees. The performance with "noise" or changed class
labels for various percentages of the data was also tested since it was
noted that AdaBoost did not do well with such noise, due to amplifying
the effect of the noise. Breiman's paper, "Random Forests",
[@Breiman2001] is attributed to be the first mention and description of
the random forest technique. Random Forest classifiers are based on
decision trees which have been generated from random subsets of the
variables. The final classification is determined by majority vote of
the decision trees.

Random Forests can be used for classification (predicting a class label)
or regression (predicting a continuous value). An advantage of random
forest is its performance with noisy data, since it does not weight the
data and hence potentially amplify the weight of noise. Random forests
do not overfit data due to the randomness of the selected variables used
for the prediction. The "out of bag" estimate for a class label is the
estimate using trees that do not contain the class label, and it is a
good estimate of overall error rate. Random forests can have similar
error rates as another machine learning algorithm, AdaBoost. Rodriguez,
et. al. [@Rodriguez2006] discuss bagging, boosting, and random forest
methods and mention that AdaBoost is "equivalent to fitting an additive
logistic regression model by a stage-wise estimation procedure," and
also note that some of the math behind bounds and generalization error
have been proven. It has also been noted that AdaBoost was successful
because of imparting diversity to the ensemble, but there was a tradeoff
in accuracy [@Rodriguez2006].

### Improvements and Expansions to Random Forest

Random forest extensions include non-parametric tests of significance,
weighted voting, dynamic integration, weighted random sampling, online
random forest (streamed / partial data), and genetic algorithms
[@Fawagreh2014]. Verikas et. al [@Verikas2011] found that the random
forest method is sensitive to tuning parameters such as number of
variables selected and number of trees. Boulesteix et. al
[@Boulesteix2012] suggested modifications to the random forest approach
regarding how individual trees are built, the construction of datasets,
and how individual predictions are coalesced into one prediction.
<!--For example, conditional inference forests are random forests where the predictors of each split are tested for response and allow adjustment for different predictors. -->

For parameter tuning, if the number of trees increases with the number
of predictors, the randomization process will lead to predictors having
a good chance to be selected. Predictor selection is important. Some
predictors are not informative. The number of predictors is important in
order to moderate the effects of strong and weak predictors. The size of
trees is determined by the splitting criteria. The size of the leaves is
another parameter wherein good predictors could be overlooked if the
leaf size threshold is too small. The authors suggest sampling without
replacement to avoid bias, and sampling a certain number of observations
from multiple classes [@Boulesteix2012].

A study by Gardener et. al [@Gardner2021] combined Random Forest
methodology with PCA to improve precision. PCA is used to create new
combination vectors for dependent features in this case. It is ideal to
create a new feature when a dependency is found, but this methodology
would take the guesswork out of it and automate the creation of the new
features. The random forest technique would then incorporate the new
features. This allows the single split of a new variable that combines
multiple features. Another improvement is the fact that each tree has
unique features due to the PCA algorithm being applied on each tree, so
the trees are less correlated, which has been shown to increase
accuracy.

Rodriguez et. al [@Rodriguez2006] also attempted to improve on random
forest methods by using PCA to create new features in a "rotation"
method to classify the rotated data. The authors mention that rotation
may not be optimal for feature extraction and there are other linear
transformations/projections that could be used instead of PCA. This
study used disjoint sets of data to create different classifiers but
noted that it could be done use intersecting sets. PCA yields some zero
eigenvectors and therefore the number of components varies. This study
kept all the components rather than discard low ranking ones. The
authors found that the rotation forest method has more accurate but less
diverse (but still reasonably diverse) individual classifiers and that
contrasts with random forests having more diverse but less accurate
classifiers, concluding that the rotation method tested provided
superior results.

Breiman in 2004 [@Breiman2004] attempted to create some simple random
forest models where mathematical consistency could be proven, referring
to a paper by Yi Lin & Yongho Jeon in 2002 (published 2006) [@Lin2006],
that showed that the random forest method was a form of adaptive nearest
neighbors, as the tree or split is a form of distance measure and
therefore it is a nearest neighbor approach. The concept of strong and
weak variables was introduced, as well as attempts to create equations
to bound variance and bias for this simple model to prove consistency.
This introduces the question: problems themselves aren't always
consistent so can the solutions be consistent?

### Applications

As the size of datasets increases (defined by the number of variables
exceeding the number of observations), the performance of statistical
methods decline and machine learning methods are preferred
[@Verikas2011]. Random forests have been used in several such studies
related to biology and medicine. In applications such as these, there is
usually a relationship between response and predictor variables and
sometimes there are strong correlations between predictors. Random
forests have helped with prediction, ranking of influential variables,
and identifying variables with interactions. Other studies have found
good performance in areas such as predicting customer churn, fraud
detection, identifying bacteria and fish, and identifying eye disease
[@Verikas2011].

Lin et al [@Lin2019] analyzed a very large dataset in order to determine
risk factors for diabetes, a disease affecting 425 million adults where
chronic high blood sugar has effects such as damaging blood vessels,
organs, nerves, and other complications. A study by Beaulac and
Rosenthal [@Beaulac2019] focused on prediction of students' major and
completion status, finding they were able to predict program completion
with 78.84% accuracy and predict choice of major with 47.41% accuracy.
Moore, et. al. used the random forest method to predict conversion from
MCI (mild cognitive impairment) to AD (Alzheimer's disease) through
pairwise comparison of time series data [@Moore2019]. The authors used a
random forest on time series data with four data points and 60 trees,
yielding 90% accuracy for the classification of neuroimage data of AD vs
HC (healthy controls).

## Methods

```{=html}
<!--
The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.
-->
```
## Analysis and Results

### Data and Visualization

Data Source:
<a href="https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset" target="_blank">Diabetes
Health Indicators Dataset (Kaggle)</a>

This dataset focuses on diabetes, a chronic health condition affecting
millions of people worldwide. Diabetes can cause serious complications
and lower quality of life; therefore, there is a need to try to predict
an individual's risk of developing diabetes in the hopes that preventive
measures can be taken to improve outcomes.[@DiabetesDataset]

This dataset is a subset of the The Behavioral Risk Factor Surveillance
System (BRFSS) dataset. The BRFSS is a survey conducted by the CDC to
examine Americans' responses in the areas of health behaviors and
conditions. The diabetes dataset includes 21 feature variables and
70,692 survey responses. The response variable is whether or not an
individual has diabetes (indicated by 0 for no or 1 for pre-diabetes or
diabetes). This dataset has a 50-50 binary split, meaning the data is
evenly split between having diabetes or not.[@DiabetesDataset]

| Data Field           | Description of Data                                                                                               |
|-------------------|-----------------------------------------------------|
| HighBP               | Told by a healthcare professional they have high blood pressure.                                                  |
| HighChol             | Told by a healthcare professional they have high cholesterol.                                                     |
| CholCheck            | Cholesterol check within past 5 years.                                                                            |
| BMI                  | Body Mass Index                                                                                                   |
| Smoker               | Smoked at least 100 cigarettes lifetime.                                                                          |
| Stroke               | Told by a healthcare professional they have had a stroke.                                                         |
| HeartDiseaseorAttack | Reported having heart disease or heart attack.                                                                    |
| PhysActivity         | Physical activity/exercise in past 30 days.                                                                       |
| Fruits               | Consume fruit 1+ times daily.                                                                                     |
| Veggies              | Consume vegetables 1+ times daily.                                                                                |
| HvyAlcoholConsump    | Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week) |
| AnyHealthcare        | Reported having any kind of health insurance/healthcare coverage.                                                 |
| NoDocbcCost          | Reported needing to see a doctor in the past year, but unable because of cost.                                    |
| GenHlth              | Statement of general health.                                                                                      |
| MentHlth             | Statement of mental health in past 30 days (not good).                                                            |
| PhysHlth             | Statement of physical health in past 30 days (not good).                                                          |
| DiffWalk             | Difficulty walking up or down stairs.                                                                             |
| Sex                  | Sex/gender                                                                                                        |
| Age                  | Age group                                                                                                         |
| Education            | Highest grade completed.                                                                                          |
| Income               | Annual household income.                                                                                          |

: Data Fields and Description

```{r}
library(tidyverse)
library(dplyr)
library(rpart)
library(rpart.plot)
library(caTools)
library(caret)

randseed <- 123456
set.seed(randseed)

#data types for data frame columns
coltypes <- c("f",#Diabetes_012
              "f",#HighBP
              "f",#HighChol
              "f",#CholCheck
              "n",#BMI
              "f",#Smoker
              "f",#Stroke
              "f",#HeartDiseaseorAttack
              "f",#PhysActivity
              "f",#Fruits
              "f",#Veggies
              "f",#HvyAlcoholConsump
              "f",#AnyHealthcare
              "f",#NoDocbcCost
              "n",#GenHlth
              "n",#MentHlth
              "n",#PhysHlth
              "f",#DiffWalk
              "f",#Sex
              "f",#Age
              "f",#Education
              "f" #Income
              )
coltypes_collapsed <- paste(coltypes, collapse="")

#read the full dataset into a dataframe - 50/50 split dataset
df <- read_csv("diabetes_binary_5050split_health_indicators_BRFSS2015.zip", col_types = coltypes_collapsed)

print("Summary of variables in the dataset")
summary(df)

#Split dataset into training (75%) and testing (25%)
set.seed(123456)
sample_split <- sample.split(Y = df$Diabetes_binary, SplitRatio = 0.75)
train_set <- subset(x = df, sample_split == TRUE)
test_set <- subset(x = df, sample_split == FALSE)

#Set up the tree with the training data
tree <- rpart(Diabetes_binary ~ HighBP + HighChol + CholCheck + BMI + Smoker + Stroke + HeartDiseaseorAttack + PhysActivity + Fruits + Veggies + HvyAlcoholConsump + AnyHealthcare + NoDocbcCost + GenHlth + MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + Income, data = train_set, method = "class")
print("The steps the classifier took to split the data:")
tree

#Display the decision tree
rpart.plot(tree, main = "Decision Tree for the Diabetes Dataset")

#Display the importance of each variable
print("Importance of each predictor variable")
importances <- varImp(tree)
importances %>% arrange(desc(Overall))

#Use this information to predict a new model
preds <- predict(tree, newdata = test_set, type = "class")
#preds

#Display accuracy data
#print("Accuracy for predicted model")
#confusionMatrix(test_set$Diabetes_binary, preds)

#create this function variable so it is easier to change it as needed
model_fn <- Diabetes_binary ~ HighBP + GenHlth + BMI + HighChol

#create model.  note rpart.control cp value needed for full dataset due to class imbalance
dt_model <- rpart(model_fn, data = df, method="class",
                  control = rpart.control(minsplit=2, cp=0.0000001, maxdepth = 4))
rpart.plot(dt_model, box.palette = "Greens", main="Decision Tree", extra = 106)

```

````{=html}
<!--
A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```
-->
````

### Statistical Modeling

### Conlusion

## References

<!-- references print automatically at bottom of page -->
