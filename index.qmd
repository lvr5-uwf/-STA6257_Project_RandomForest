---
title: "Random Forest (Decision Trees)"
#author ref https://quarto.org/docs/journals/authors.html
author: 
- name: "Danielle Novinski"
- name: "Victor Richerson"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

This is an introduction to random forests, a machine learning method widely used in a variety of applications such as ecology, medicine, astronomy, agriculture, traffic, and bioinformatics [@Fawagreh2014]. Random forest is an ensemble learning method composed of multiple decision trees. Breiman [@Breiman2001] proposed random forests to improve on decision trees for regression and classification tasks. Since this initial paper, many studies have been proposed to improve and extend the random forest method as well as use random forest for classification and regression techniques in a variety of applications. Our paper will provide an overview of the existing research and methods used.

## Literature Review

### Background

Decision trees are used for prediction and classification in a variety of artificial intelligence and machine learning tasks. Decision trees divide data by fields, creating subsets called nodes. Cutoffs are applied based on statistics [@deVille2013]. Researchers have sought to make improvements to decision tree classifiers, such as reducing bias and variance. Resampling training data and using multiple trees reduces bias and variance [@deVille2013]. Bootstrap aggregating (bagging) and AdaBoost were improvements over the standard decision tree. In addition, other multi-tree solutions came before the random forest method, including voting over multiple trees and using split methods of alternative splits and oblique splits [@Breiman1996]. The Random Forest method is an ensemble method. Ensemble methods use several methods to solve a problem. Diversity among methods is indicated by methods making different errors. This leads to improvements in classifications [@Fawagreh2014]. 

Dietterich [@Dietterich2000] compared a method of randomizing the split of C4.5 decision trees with AdaBoost and Bagging methods on 33 datasets. This study discussed the effect of randomization leading to a diverse set of decision trees. The performance with "noise" or changed class labels for various percentages of the data was also tested since it was noted that AdaBoost did not do well with such noise, due to amplifying the effect of the noise. Breiman's paper, "Random Forests", [@Breiman2001] is attributed to be the first mention and description of the random forest technique.  Random Forest classifiers are based on decision trees which have been generated from random subsets of the variables.  The final classification is determined by majority vote of the decision trees.  

Random Forests can be used for classification (predicting a class label) or regression (predicting a continuous value).  An advantage of random forest is its performance with noisy data, since it does not weight the data and hence potentially amplify the weight of noise.  Random forests do not overfit data due to the randomness of the selected variables used for the prediction.  The "out of bag" estimate for a class label is the estimate using trees that do not contain the class label, and it is a good estimate of overall error rate.  Random forests can have similar error rates as another machine learning algorithm, AdaBoost. Rodriguez, et. al. [@Rodriguez2006] discuss bagging, boosting, and random forest methods and mention that AdaBoost is "equivalent to fitting an additive logistic regression model by a stage-wise estimation procedure," and also note that some of the math behind bounds and generalization error have been proven. It has also been noted that AdaBoost was successful because of imparting diversity to the ensemble, but there was a tradeoff in accuracy [@Rodriguez2006].  

### Improvements and Expansions to Random Forest

Random forest extensions include non-parametric tests of significance, weighted voting, dynamic integration, weighted random sampling, online random forest (streamed / partial data), and genetic algorithms [@Fawagreh2014]. Verikas et. al [@Verikas2011] found that the random forest method is sensitive to tuning parameters such as number of variables selected and number of trees. Boulesteix et. al [@Boulesteix2012] suggested modifications to the random forest approach regarding how individual trees are built, the construction of datasets, and how individual predictions are coalesced into one prediction. <!--For example, conditional inference forests are random forests where the predictors of each split are tested for response and allow adjustment for different predictors. -->

For parameter tuning, if the number of trees increases with the number of predictors, the randomization process will lead to predictors having a good chance to be selected. Predictor selection is important. Some predictors are not informative. The number of predictors is important in order to moderate the effects of strong and weak predictors. The size of trees is determined by the splitting criteria. The size of the leaves is another parameter wherein good predictors could be overlooked if the leaf size threshold is too small. The authors suggest sampling without replacement to avoid bias, and sampling a certain number of observations from multiple classes [@Boulesteix2012]. 

A study by Gardener et. al [@Gardner2021] combined Random Forest methodology with PCA to improve precision.  PCA is used to create new combination vectors for dependent features in this case.  It is ideal to create a new feature when a dependency is found, but this methodology would take the guesswork out of it and automate the creation of the new features.  The random forest technique would then incorporate the new features.  This allows the single split of a new variable that combines multiple features.  Another improvement is the fact that each tree has unique features due to the PCA algorithm being applied on each tree, so the trees are less correlated, which has been shown to increase accuracy.

Rodriguez et. al [@Rodriguez2006] also attempted to improve on random forest methods by using PCA to create new features in a "rotation" method to classify the rotated data. The authors mention that rotation may not be optimal for feature extraction and there are other linear transformations/projections that could be used instead of PCA. This study used disjoint sets of data to create different classifiers but noted that it could be done use intersecting sets. PCA yields some zero eigenvectors and therefore the number of components varies.  This study kept all the components rather than discard low ranking ones. The authors found that the rotation forest method has more accurate but less diverse (but still reasonably diverse) individual classifiers and that contrasts with random forests having more diverse but less accurate classifiers, concluding that the rotation method tested provided superior results.

Breiman in 2004 [@Breiman2004] attempted to create some simple random forest models where mathematical consistency could be proven, referring to a paper by Yi Lin & Yongho Jeon in 2002 (published 2006) [@Lin2006], that showed that the random forest method was a form of adaptive nearest neighbors, as the tree or split is a form of distance measure and therefore it is a nearest neighbor approach. The concept of strong and weak variables was introduced, as well as attempts to create equations to bound variance and bias for this simple model to prove consistency.  This introduces the question: problems themselves arenâ€™t always consistent so can the solutions be consistent?

### Applications

As the size of datasets increases (defined by the number of variables exceeding the number of observations), the performance of statistical methods decline and machine learning methods are preferred [@Verikas2011]. Random forests have been used in several such studies related to biology and medicine. In applications such as these, there is usually a relationship between response and predictor variables and sometimes there are strong correlations between predictors. Random forests have helped with prediction, ranking of influential variables, and identifying variables with interactions. Other studies have found good performance in areas such as predicting customer churn, fraud detection, identifying bacteria and fish, and identifying eye disease [@Verikas2011].

Lin et al [@Lin2019] analyzed a very large dataset in order to determine risk factors for diabetes, a disease affecting 425 million adults where chronic high blood sugar has effects such as damaging blood vessels, organs, nerves, and other complications. A study by Beaulac and Rosenthal [@Beaulac2019] focused on prediction of students' major and completion status, finding they were able to predict program completion with 78.84% accuracy and predict choice of major with 47.41% accuracy. Moore, et. al. used the random forest method to predict conversion from MCI (mild cognitive impairment) to AD (Alzheimer's disease) through pairwise comparison of time series data  [@Moore2019]. The authors used a random forest on time series data with four data points and 60 trees, yielding 90% accuracy for the classification of neuroimage data of AD vs HC (healthy controls). 



## Methods

<!--
The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.
-->

## Analysis and Results

### Data and Vizualisation

Data Source: <a href="https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset" target="_blank">Diabetes Health Indicators Dataset (Kaggle)</a>

```{r}
library(tidyverse)
library(dplyr)
library(rpart)
library(rpart.plot)

randseed <- 123456
set.seed(randseed)

#data types for data frame columns
coltypes <- c("f",#Diabetes_012
              "f",#HighBP
              "f",#HighChol
              "f",#CholCheck
              "n",#BMI
              "f",#Smoker
              "f",#Stroke
              "f",#HeartDiseaseorAttack
              "f",#PhysActivity
              "f",#Fruits
              "f",#Veggies
              "f",#HvyAlcoholConsump
              "f",#AnyHealthcare
              "f",#NoDocbcCost
              "n",#GenHlth
              "n",#MentHlth
              "n",#PhysHlth
              "f",#DiffWalk
              "f",#Sex
              "f",#Age
              "f",#Education
              "f" #Income
              )
coltypes_collapsed <- paste(coltypes, collapse="")

#read the full dataset into a dataframe - 50/50 split dataset
df <- read_csv("diabetes_binary_5050split_health_indicators_BRFSS2015.zip", col_types = coltypes_collapsed)

#create this function variable so it is easier to change it as needed
model_fn <- Diabetes_binary ~ HighBP + GenHlth + BMI + HighChol

#create model.  note rpart.control cp value needed for full dataset due to class imbalance
dt_model <- rpart(model_fn, data = df, method="class",
                  control = rpart.control(minsplit=2, cp=0.0000001, maxdepth = 4))
rpart.plot(dt_model, box.palette = "Greens", main="Decision Tree", extra = 106)

```

<!--
A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```
-->

### Statistical Modeling

### Conlusion

## References
<!-- references print automatically at bottom of page -->